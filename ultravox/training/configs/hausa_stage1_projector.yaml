# Stage 1: Train only the adapter/projector
# Audio tower and LLM are frozen by default

text_model: "meta-llama/Llama-3.3-70B-Instruct"
audio_model: "openai/whisper-large-v3"

# Dataset configuration
# Use HuggingFace datasets (from vaghawan/hausa-audio-resampled)
train_sets:
  - name: hausa-huggingface-train
val_sets:
  - name: hausa-huggingface-val

loss_config:
  loss_function: "KL_Divergence"

# Training configuration - Epoch-based training
max_steps: 120000 # 20 epochs
batch_size: 24  # Per-GPU batch size for 4x A100 80GB with FSDP
# Effective batch size = batch_size * num_gpus = 24 * 4 = 96

# Learning rate configuration for projector-only training
# Since only projector is trainable (audio tower & LLM frozen), higher LR is safe
lr: 5e-4  # Higher LR for projector-only (vs 1e-4 for full training, 5e-5 for LoRA)
lr_warmup_steps: 1000  # Warmup steps for stability with 70B model

# Multi-GPU configuration for 4x A100 GPUs
use_fsdp: true  # Required for Llama-3.3-70B on 4x A100 GPUs

# Logging and saving configuration
logging_steps: 1000  # Log metrics every 10 steps (integer = exact steps)
save_steps: 5000  # Save checkpoint every 500 steps (integer = exact steps)
val_steps: 5000  # Evaluate every 500 steps (integer = exact steps)

# Validation configuration
val_batch_size: 24  # Match training batch size (per GPU)
do_eval: false  # Evaluation not supported with FSDP - set to false

report_logs_to: ["wandb"]
output_dir: "./output"

# HuggingFace Hub upload configuration
# Upload checkpoints to Hub after each save_steps (overrides previous uploads)
hub_upload_enabled: true
hub_repo: "vaghawan/hausa-ultravox-stage1"
hub_best_tag: "best"
hub_last_tag: "last"
hub_upload_best: true
hub_upload_last: true

