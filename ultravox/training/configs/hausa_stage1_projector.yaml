# Stage 1: Train only the adapter/projector
# Audio tower and LLM are frozen by default

text_model: "zai-org/GLM-4.6"
audio_model: "openai/whisper-large-v3-turbo"

# Load pre-trained Ultravox checkpoint
model_load_dir: "fixie-ai/ultravox-v0_7-glm-4_6"

# Only projector will be trained (default behavior)
# No LoRA configs = audio_tower and language_model are frozen

train_sets:
  - name: hausa-train
val_sets:
  - name: hausa-val

# Training configuration - Epoch-based training
num_epochs: 100  # Train for 100 epochs
max_steps: -1 # -1 means use num_epochs instead of max_steps
batch_size: 10
grad_accum_steps: 2
lr: 5e-4
lr_warmup_steps: 1000

# Data loading configuration
num_workers: 8  # Parallel data loading workers (default: 8 if CUDA available, 1 otherwise)

# Logging and saving configuration
# With 2000 samples and batch_size=10: steps_per_epoch = 2000/10 = 200 steps
logging_steps: 0.1  # Log metrics every 100 steps (twice per epoch)
save_steps: 0.1  # Save checkpoint every epoch (200 steps = 1 epoch)

# Validation configuration
val_steps: 0.5  # Validate once per epoch (200 steps = 1 epoch with your dataset)
val_batch_size: 8
do_eval: true  # Enable evaluation on val_sets during training

# FSDP configuration
use_fsdp: false