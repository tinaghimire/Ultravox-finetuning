# Stage 1: Train only the adapter/projector
# Audio tower and LLM are frozen by default

text_model: "zai-org/GLM-4.6"
audio_model: "openai/whisper-large-v3-turbo"

# Only projector will be trained (default behavior)
# No LoRA configs = audio_tower and language_model are frozen

# Dataset configuration
# Use HuggingFace datasets (from vaghawan/hausa-audio-resampled)
train_sets:
  - name: hausa-huggingface-train
val_sets:
  - name: hausa-huggingface-val
test_sets:
  - name: hausa-huggingface-test

# Alternative: Use offline datasets (local files)
# train_sets:
#   - name: hausa-offline-train
# val_sets:
#   - name: hausa-offline-val
# test_sets:
#   - name: hausa-offline-test

# Training configuration - Epoch-based training
num_epochs: 100  # Train for 100 epochs
max_steps: 0  # 0 means use num_epochs; > 0 overrides num_epochs
batch_size: 32  # Increased from 10 to 32 for better GPU utilization
grad_accum_steps: 2  # Effective batch size = 32 * 2 = 64
lr: 5e-4
lr_warmup_steps: 1000

# Data loading configuration
num_workers: 8  # Parallel data loading workers

# Logging and saving configuration
# Note: logging_steps, save_steps, val_steps can be:
#   - Integer: exact number of steps
#   - Float (0-1): fraction of steps per epoch
# Example: If steps_per_epoch=100, logging_steps=0.1 means every 10 steps
logging_steps: 100  # Log metrics every 10 steps (integer = exact steps)
save_steps: 500  # Save checkpoint every 500 steps (integer = exact steps)

# Validation configuration
val_steps: 500  # Validate every 500 steps (integer = exact steps)
val_batch_size: 32  # Match training batch size
do_eval: true  # Enable evaluation on val_sets during training

# FSDP configuration
# FSDP (Fully Sharded Data Parallel) is better for training because:
# 1. Memory efficiency: Shards model parameters across GPUs, reducing memory per GPU
# 2. Scalability: Can train larger models with limited GPU memory
# 3. Performance: Better GPU utilization for large models
# 4. Gradient synchronization: Efficient gradient communication across GPUs
# Enable if training on multiple GPUs with large models
use_fsdp: true  # Set to true for multi-GPU training with large models