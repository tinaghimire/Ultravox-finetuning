# Stage 3: Fine-tune LLM using LoRA
# Load checkpoint from Stage 2, then add LoRA to LLM

text_model: "zai-org/GLM-4.6"
audio_model: "openai/whisper-large-v3-turbo"

# Load checkpoint from Stage 2 (update path after Stage 2 completes)
model_load_dir: "outputs/hausa_stage2_audio_lora/checkpoint-<num_steps>"  # Update with actual checkpoint path

# Keep audio LoRA from Stage 2 (loaded from checkpoint)
audio_model_lora_config:
  r: 8
  lora_alpha: 16
  target_modules:
    - "k_proj"
    - "q_proj"
    - "v_proj"
    - "out_proj"

# Enable LoRA for LLM (GLM-4.6)
text_model_lora_config:
  r: 16                   # Higher rank for LLM (more capacity)
  lora_alpha: 32         # Typically 2x rank
  target_modules:        # LLM attention modules
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

train_sets:
  - name: hausa-train
val_sets:
  - name: hausa-val

batch_size: 6            # Smaller batch due to more trainable parameters
max_steps: 500
lr: 2e-5                 # Even lower learning rate for LLM fine-tuning
lr_warmup_steps: 50
save_steps: 100
logging_steps: 50

# Validation configuration
val_steps: 1.0  # Validate every epoch (when using num_epochs) or set to steps_per_epoch
val_batch_size: 4
do_eval: true  # Enable evaluation on val_sets during training

